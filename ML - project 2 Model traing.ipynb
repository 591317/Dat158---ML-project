{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size start (rows, columns):\n",
      "(188533, 9)\n",
      "Test size after engine info (rows, columns):\n",
      "(188533, 9)\n",
      "Test size after accident info (rows, columns):\n",
      "(188533, 9)\n",
      "Test size after brand-merge info (rows, columns):\n",
      "(188533, 101)\n",
      "Root Mean Squared Error (RMSE): 72443.9212\n"
     ]
    }
   ],
   "source": [
    "# ML - project 2 Model traing\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import re\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "\n",
    "# different Model imports\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# import metrics for evaluating the model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# import for splitting the data so we get 75 % train-data and 25 %  test-data'\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Trun categories into number\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# used to reduced the datas compelixty\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "# Setps for machine work-flow\n",
    "# 1.Read the Data: Load the data from a CSV or other data source\n",
    "# 2.Preprocess the Data: Clean the data, handle missing values, encode categorical variables, scale numerical features, etc.\n",
    "# 3.Split the Data: Use train_test_split to divide the data into training and testing sets.\n",
    "# 4.Train the Model(s): Fit your chosen machine learning model(s) on the training data X_train and y_train.\n",
    "# 5.Evaluate the Model(s): Use the testing data X_test and y_test to evaluate the performance of your model(s).\n",
    "\n",
    "file_path = r'C:\\Users\\Eric\\OneDrive\\Skrivebord\\Dat158 - ML project 2\\train.csv'\n",
    "# Read the CSV file into a DataFrame\n",
    "train_df = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess the Data: Clean the data\n",
    "# removed id and price form the data frame. Make the columns with string values to integers.\n",
    "\n",
    "#print(train_df['brand'].value_counts())\n",
    "#print(train_df['ext_col'].value_counts())\n",
    "#print(train_df['transmission'].value_counts())\n",
    "#print(train_df['model'].value_counts())\n",
    "#print(train_df['fuel_type'].value_counts())\n",
    "\n",
    "\n",
    "# Split the Data: 75 - train og 25 test\n",
    "# target_variable is the column that holdes the price in the data-frame. id_column holdes the id values.\n",
    "# make these as variables, so we can easy swap them out later with something else if needed or use them to edit there format.\n",
    "target_variable = 'price'\n",
    "id_column = 'id'\n",
    "brand_column = 'brand'\n",
    "modelName_column = 'model'\n",
    "fuelType_colum = 'fuel_type'\n",
    "engine_column = 'engine'\n",
    "transmission_column = 'transmission'\n",
    "extColor_column = 'ext_col'\n",
    "intColor_column = 'int_col'\n",
    "accident_column = 'accident'\n",
    "cleanTile_column = 'clean_title'\n",
    "\n",
    "\n",
    "# Separate features and target in the training data\n",
    "X = train_df.drop(columns=[target_variable, id_column,intColor_column,cleanTile_column])\n",
    "y = train_df[target_variable]\n",
    "\n",
    "print(\"Test size start (rows, columns):\")\n",
    "print(X.shape)\n",
    "\n",
    "# Extract  horsepower for egnine column using regex\n",
    "def extract_engine_info(engine_str):\n",
    "    if pd.isnull(engine_str):\n",
    "        return np.nan\n",
    "    hp_match = re.search(r'(\\d+\\.?\\d*)HP', engine_str)\n",
    "    hp = float(hp_match.group(1)) if hp_match else np.nan\n",
    "    return hp\n",
    "\n",
    "# Applying the function to the engine column\n",
    "X[engine_column] = X[engine_column].apply(extract_engine_info)\n",
    "\n",
    "\n",
    "# Fill NaNs in the engine column with the mean value of the column\n",
    "X['engine'].fillna(X['engine'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "print(\"Test size after engine info (rows, columns):\")\n",
    "print(X.shape)\n",
    "\n",
    "# Define the function for checking the accident information\n",
    "def accident_info(accident_str):\n",
    "    # Check if the string is None or NaN\n",
    "    if pd.isnull(accident_str):\n",
    "        return 0  # Assuming NaN means no report, hence 0\n",
    "    return 0 if accident_str.strip().lower() == 'none reported' else 1\n",
    "\n",
    "# Applying the function to the accident_column\n",
    "X[accident_column] = X[accident_column].apply(accident_info)\n",
    "\n",
    "\n",
    "print(\"Test size after accident info (rows, columns):\")\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "# set a threshold for the minimum number of occurenences for a brand to be a seperate category\n",
    "threshold_brand = 500\n",
    "\n",
    "# Count occurrences for each brand\n",
    "brand_counts = X[brand_column].value_counts()\n",
    "\n",
    "# Identify brands that are common\n",
    "commond_brand = brand_counts[brand_counts >= threshold_brand].index\n",
    "\n",
    "# teste kun for en bil type audi: X.where(X[brand_column].isin('audi')) ???????????????\n",
    "# Group the less frequent brans into 'other' using .where()\n",
    "X[brand_column] = X[brand_column].where(X[brand_column].isin(commond_brand), 'other')\n",
    "\n",
    "# One-hot encode the 'brand' column so we dont use the String-values\n",
    "X = pd.get_dummies(X, columns=[brand_column])\n",
    "\n",
    "\n",
    "# set a threshold for the minimum number of occurenences for a color to be a seperate category\n",
    "threshold_extcolor = 1000\n",
    "\n",
    "# Count occurrences for each brand\n",
    "extColor_counts = X[extColor_column].value_counts()\n",
    "\n",
    "# Identify color that are common\n",
    "commond_color = extColor_counts[extColor_counts >= threshold_extcolor].index\n",
    "\n",
    "# Group the less frequent brans into 'other' using .where()\n",
    "X[extColor_column] = X[extColor_column].where(X[extColor_column].isin(commond_color), 'other')\n",
    "\n",
    "# One-hot encode the 'ext_col' column so we dont use the String-values\n",
    "X = pd.get_dummies(X, columns=[extColor_column])\n",
    "\n",
    "\n",
    "# set a threshold for the minimum number of occurenences for a transmission to be a seperate category\n",
    "threshold_transmission = 500\n",
    "\n",
    "# Count occurrences for each transmission\n",
    "transmission_counts = X[transmission_column].value_counts()\n",
    "\n",
    "# Identify transmission that are common\n",
    "commond_transmission = transmission_counts[transmission_counts >= threshold_extcolor].index\n",
    "\n",
    "# Group the less frequent transmission into 'other' using .where()\n",
    "X[transmission_column] = X[transmission_column].where(X[transmission_column].isin(commond_transmission), 'other')\n",
    "\n",
    "# One-hot encode the 'transmission' column so we dont use the String-values\n",
    "X = pd.get_dummies(X, columns=[transmission_column])\n",
    "\n",
    "\n",
    "# set a threshold for the minimum number of occurenences for a model-name to be a seperate category\n",
    "threshold_modelName = 500\n",
    "\n",
    "# Count occurrences for each transmission\n",
    "modelName_counts = X[modelName_column].value_counts()\n",
    "\n",
    "# Identify transmission that are common\n",
    "commond_modelName = modelName_counts[modelName_counts >= threshold_extcolor].index\n",
    "\n",
    "# Group the less frequent transmission into 'other' using .where()\n",
    "X[modelName_column] = X[modelName_column].where(X[modelName_column].isin(commond_modelName), 'other')\n",
    "\n",
    "# One-hot encode the 'transmission' column so we dont use the String-values\n",
    "X = pd.get_dummies(X, columns=[modelName_column])\n",
    "\n",
    "\n",
    "# set a threshold for the minimum number of occurenences for a fuel-type to be a seperate category\n",
    "threshold_fuelType = 500\n",
    "\n",
    "# Count occurrences for each fuel-type\n",
    "fuelType_counts = X[fuelType_colum].value_counts()\n",
    "\n",
    "# Identify fuel-type that are common\n",
    "commond_fuelType = fuelType_counts[fuelType_counts >= threshold_extcolor].index\n",
    "\n",
    "# Group the less frequent fuel-type into 'other' using .where()\n",
    "X[fuelType_colum] = X[fuelType_colum].where(X[fuelType_colum].isin(commond_fuelType), 'other')\n",
    "\n",
    "# One-hot encode the 'fuel_type' column so we dont use the String-values\n",
    "X = pd.get_dummies(X, columns=[fuelType_colum])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" # Initialize TargetEncoder\n",
    "encoder = TargetEncoder(cols=[brand_column])\n",
    "\n",
    "# Fit and transform the feature dataframe\n",
    "X_transformed = encoder.fit_transform(X, y)\n",
    "\n",
    "print(\"Test size after encoder (rows, columns):\")\n",
    "print(X_transformed.shape)\n",
    "\n",
    "# Adding the transformed column back to the original dataframe\n",
    "X[brand_column] = X_transformed[brand_column]\n",
    "\n",
    "print(X[brand_column].head(10)) \"\"\"\n",
    "\n",
    "print(\"Test size after brand-merge info (rows, columns):\")\n",
    "print(X.shape)\n",
    "\n",
    "# Specify the categorical features\n",
    "\"\"\" categorical_features = [modelName_column, fuelType_colum, transmission_column,extColor_column] \"\"\"\n",
    "\n",
    "# Initialize and configure the transformer\n",
    "\"\"\" one_hot = OneHotEncoder(handle_unknown='ignore')\n",
    "transformer = ColumnTransformer([(\"one_hot\",\n",
    "                                 one_hot,\n",
    "                                 categorical_features)],\n",
    "                                 remainder=\"passthrough\")\n",
    "\n",
    "print(\"Test size before splitt (rows, columns):\")\n",
    "print(X.shape) \"\"\"\n",
    "\n",
    "# splitting the data into train and test sets (75% train and 25% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Fit the transformer on the training data\n",
    "\"\"\" transformer.fit(X_train) \"\"\"\n",
    "\n",
    "# Transform both the training and test data using the fitted transformer\n",
    "\"\"\" X_train_tranformed = transformer.fit_transform(X_train)\n",
    "X_test_tranformed = transformer.transform(X_test) \"\"\"\n",
    "\n",
    "\n",
    "# Create an instance of a model:\n",
    "ln_reg = LinearRegression()\n",
    "rf_reg = RandomForestRegressor()\n",
    "gb_reg = GradientBoostingRegressor()\n",
    "\n",
    "# Train different models with .fit:\n",
    "#ln_reg.fit(X_train, y_train)\n",
    "#rf_reg.fit(X_train, y_train)\n",
    "gb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make a prediction of the test\n",
    "#y_pred = ln_reg.predict(X_test)\n",
    "#y_pred = rf_reg.predict(X_test)\n",
    "y_pred = gb_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "#rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    " \n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
